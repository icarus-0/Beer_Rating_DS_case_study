# -*- coding: utf-8 -*-
"""Algoscale_Beer_rating_DS_casestudy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hLE0rVqoYHLzRvdT3cUQMIBzVmsGNWd3

**ROUND : DATA SCIENCE CASE-STUDY** 

                                      Name - Naman Singh Chandel
                                      unroll- 171500202
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing some basic libraries for Loading the  data and for data visualization

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

data = pd.read_csv("/content/drive/My Drive/train.csv") # importing the Beer Rating dataset
data.head()

"""visualing Data Heatmap by using Seaborn"""

sns.heatmap(data.isnull(),cbar=False,cmap='viridis')

"""By this Heatmap we can know in which column the data have null values

**EDA : Exploratory Data Analysis Part**
"""

b = sns.PairGrid(data)
b.map(plt.scatter)

sns.relplot(x='beer/ABV',y='review/overall',data=data,kind='line')

sns.relplot(x='review/aroma',y='review/overall',data=data)

sns.catplot(x='review/aroma',y='review/overall',data=data)

"""**Feature Seletion Part**


In this Data 'user/ageInSeconds','user/birthdayRaw','user/birthdayUnix','user/gender', are having more than 70% null value(Imputation may cause Imbancing in data, so we can drop these columns)

'index','beer/beerId','beer/brewerId','beer/name','review/timeUnix','user/profileName'

And these columns are not effecting the rating of the Beer so can be droped
"""

data = data.drop(['user/ageInSeconds','user/birthdayRaw','user/birthdayUnix','user/gender','index','beer/beerId','beer/brewerId','beer/name','review/timeUnix','user/profileName'],axis=1)
data.shape

"""**Feature Engineering Part**

From 'review/timeStruct' extracting the month because in month can affect the rating like in summer the beer rating may can less and in winter it can be high
"""

month = []
for i in range(data['review/timeStruct'].shape[0]):
  s = data['review/timeStruct'][i]
  start = s.find('mon')   #finding the index of 'mon' because after this string the month is provided
  val = s[start+5:start+8] 
  if val[2]==',': #if the sliced string have ',' it mean its a single digit number so again str will sliced
    val = val[:2]
  month.append(int(val)) # converting into integer and making a month list

month = pd.Series(month)
data['review_month'] = month # adding the month to the data
data = data.drop(['review/timeStruct'],axis=1)
data.head()

"""Now from 'review/text' using Natural Language Processing tecnique to create a sparse matrix of reviews and predicting the 'review/overall' to got to know about how a review is relate and what is the rating of a review"""

from sklearn.feature_extraction.text import CountVectorizer #Importing CountVectorizer for creating sparse matrix
import re
import string

text = data[['review/overall','review/text']] # creating a seprate DataFrame to work on

x = text['review/text']
y = text['review/overall']

alltext = list(text['review/text'])


text.rename({'review/text':'review'})



def clean_text_round1(text):
  try:
    text = text.lower() # Converting the reviews to lower case
    text = re.sub('\[.*?\]','',text)  # removing unnecessary symbols
    text = re.sub('[%s]'% re.escape(string.punctuation),'',text)
    text = re.sub('\w*\d\w*','',text) # taking only str values
    return text
  except:
    pass
round1 = lambda x: clean_text_round1(x)

dataclean = pd.DataFrame(text['review/text'].apply(round1))

import nltk
nltk.download('punkt')
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

a = list(dataclean['review/text'])

clean=[]
ps = PorterStemmer()
for i in a:
  try:
    words = word_tokenize(i)
    z = []
    for w in words:
      z.append(ps.stem(w)) # Stemming each word 
    s = ' '
    res = s.join(z) # making a clean review
    clean.append(res)
  except:
    clean.append(i)

s = pd.Series(clean)

text['reviews'] = s
text = text.drop(['review/text'],axis=1)

x = text['reviews']
y = text['review/overall']

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(y) # Encoding the 'review/overall' for classification model

cv  = CountVectorizer(max_features=525)
features  = cv.fit_transform(x.values.astype('U')) # Creating Sparse Matrix of reviews

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

clf.fit(features,y)

predict = clf.predict(features)

text_review = pd.Series(predict)
data['text_review'] = text_review
data = data.drop(['review/text'],axis=1)
data.head()

data['beer/style'] = encoder.fit_transform(data['beer/style'])

data['review/overall'] = encoder.fit_transform(data['review/overall'])

from sklearn.model_selection import train_test_split
Y = data['review/overall'].values
X = data.drop(['review/overall'],axis=1)
trainx,textx,trainy,testy=train_test_split(X,Y,test_size=.2,random_state=0)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()

clf.fit(trainx,trainy)

predict = clf.predict(textx)

p = list(predict)
l = list(testy)
count = 0
for i in range(len(p)):
  if p[i] == l[i]:
    count += 1

decision_tree_acc = count/len(testy)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

clf.fit(trainx,trainy)

predict = clf.predict(textx)

p = list(predict)
l = list(testy)
count = 0
for i in range(len(p)):
  if p[i] == l[i]:
    count += 1

random_forest_acc = count/len(testy)

from sklearn import svm
clf = svm.SVC()

clf.fit(trainx,trainy)

predict = clf.predict(textx)
p = list(predict)
l = list(testy)
count = 0
for i in range(len(p)):
  if p[i] == l[i]:
    count += 1
svm_acc = count/len(testy)

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(trainx,trainy)

predict = clf.predict(textx)
p = list(predict)
l = list(testy)
count = 0
for i in range(len(p)):
  if p[i] == l[i]:
    count += 1

GaussianNb_ac = count/len(testy)

acc_dict = {'ALOGORITHM_NAME':['Decision Tree','Random Forest','SVM','GaussianNB'],'Accuracy':[decision_tree_acc*100,random_forest_acc*100,svm_acc*100,GaussianNb_ac*100]}
Accuracy = pd.DataFrame.from_dict(acc_dict)

Accuracy

